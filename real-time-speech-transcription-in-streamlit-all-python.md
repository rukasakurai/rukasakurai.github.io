> [!WARNING] Generated by AI, and has not been reviewed in detail
# Real-Time Speech Transcription in Streamlit (All-Python Approach)

## Microphone Streaming in Streamlit

**Yes, it’s feasible to stream microphone audio and display live transcribed text in a Streamlit app**, using Python libraries and Streamlit components (without writing custom JavaScript). Streamlit by itself doesn’t natively stream audio from the browser, but you can leverage a component like **`streamlit-webrtc`** to capture audio from the user’s browser microphone in real-time and process it on the Python backend. In fact, the author of `streamlit-webrtc` created a demo that **converts your voice to text in real time** purely within a Streamlit app. This approach uses WebRTC under the hood (via `aiortc`) to transmit audio data from the browser to the Python server, where you can feed it into speech-to-text (STT) engines and update the UI continuously.

**How it works:** With `streamlit-webrtc`, your app can open a WebRTC connection to the browser. The user clicks a “Start” button (triggering browser permission for mic access), and audio frames start streaming to the server. You can specify an `audio_frame_callback` or use the provided audio receiver interface to process incoming frames. Streamlit-webrtc essentially handles the frontend (JavaScript) part for you, so you **don’t** have to write custom JS – the component captures the mic audio and sends it to your Python code in real time. This means you *can* continuously update a text element on the Streamlit app as new speech is recognized.

**Alternatives:** Aside from `streamlit-webrtc`, newer APIs and components exist, but with limitations. For example, **`st.audio_input`** (introduced in recent Streamlit versions) provides a way to record audio via the browser. However, `st.audio_input` behaves like `st.camera_input` – it records a *clip* and returns it after stopping, not a live stream. It’s useful for one-off recordings (e.g. press a button to record, then transcribe the whole clip), but not for *live, word-by-word updates*. Another community component, **`streamlit-mic-recorder`**, similarly lets you record audio and even offers a helper to run speech recognition on the result. But it doesn’t stream continuous transcripts; it’s more for “record then get text” usage. In summary, for *simultaneous streaming and transcription display*, **Streamlit-webrtc is the go-to solution** today.

## Libraries for Real-Time Transcription

Several Python libraries and STT services can plug into this setup:

* **streamlit-webrtc (and aiortc/PyAV)** – As described, this is the key enabling library for real-time media in Streamlit. It captures audio/video in the browser and makes frames available in Python callbacks. You can run it in send-only mode for audio. The official demo uses it with Mozilla DeepSpeech to achieve streaming STT entirely offline. Under the hood it uses `aiortc` and `pyav` to handle WebRTC streams.

* **Open source STT engines (offline):**

  * *Mozilla DeepSpeech:* The Streamlit-webrtc demo app used DeepSpeech to transcribe speech in real time. DeepSpeech provides a streaming API (`Model.createStream()` with `feedAudioContent` and `intermediateDecode()` for partial results). In the demo code, incoming audio frames are fed into a DeepSpeech stream and partial text is retrieved continuously.
  * *OpenAI Whisper:* Whisper is very accurate, but the open-source model doesn’t natively stream – it normally processes audio in chunks or entire files. You **can** still use Whisper in near-real-time by chunking audio input manually. For example, you might accumulate a few seconds of audio from the stream and run Whisper on that buffer asynchronously. Whisper won’t produce word-by-word partial results out-of-the-box (it outputs complete segments after some silence or buffer end), so the user experience might be “slightly delayed phrases” rather than instantaneous words. To improve this, you could use smaller Whisper models (for speed) and update the text each time a new segment is decoded. There are community attempts to get Whisper streaming (and Whisper-based APIs like OpenAI’s can return partial segments), but expect some latency.
  * *Vosk:* Vosk is another lightweight offline ASR library that supports streaming recognition with partial results. It can recognize speech in real-time on CPU and would be easier on resources than Whisper. Integrating Vosk with streamlit-webrtc is feasible — feed the audio frames to a Vosk recognizer and use its interim result callbacks.

* **Cloud STT services (online):**

  * *Azure Cognitive Services (Azure Speech SDK):* Azure’s Speech SDK for Python supports real-time transcription with low latency, including partial (interim) results. You can create a **`SpeechRecognizer`** with a **`PushAudioInputStream`** – as audio arrives from the client, write it into the stream, and Azure will continuously produce text. One community example did exactly this: they piped frames from `streamlit-webrtc` into Azure’s `AudioConfig(stream=PushAudioInputStream)` and ran the recognizer in a loop or background thread. Azure’s SDK can invoke a callback for continuous recognition or you can call `recognize_once()` repeatedly. In the example, a separate thread calls `recognize_once()` in a loop to grab each utterance and update `st.session_state` with the text. Azure yields final transcriptions quickly once the speaker pauses, and can provide intermediate results if you use the continuous mode with event handlers. This approach offloads the heavy lifting to Azure’s cloud – useful if you need high accuracy and don’t want to run a model locally.
  * *Google Cloud Speech-to-Text:* Similarly, Google’s streaming STT API (via the `google-cloud-speech` Python library) can stream mic audio and return partial transcripts. You’d stream audio chunks from the WebRTC component into Google’s API requests. This requires handling a gRPC streaming response asynchronously. Feasible with Python threads or asyncio, but adds complexity (and cost).
  * *Others:* Services like **AssemblyAI**, **Deepgram**, **IBM Watson STT**, etc., have streaming endpoints and Python SDKs or REST/websocket APIs. They can be integrated by forwarding audio frames to their API and receiving transcripts. The limitation, again, is ensuring the audio is captured from the browser – which `streamlit-webrtc` can handle.

* **Helper Libraries:**

  * *PyDub / NumPy / av:* These are used to manipulate audio frames. For example, in the DeepSpeech demo code, `av.AudioFrame` objects are converted to NumPy arrays and assembled into a chunk using PyDub before feeding to the STT model.
  * *asyncio / threading:* Since Streamlit isn’t inherently designed for long-running background tasks, you might use Python threads or async loops to run the transcription in parallel with the Streamlit app’s event loop. For instance, you can spawn a thread that waits for STT results and then updates the UI (via `st.session_state` or a placeholder). The forum example above starts a background thread for Azure recognition. Alternatively, `streamlit-webrtc` offers a callback-based approach (e.g. a `queued_audio_frames_callback`) which can collect audio frames asynchronously without blocking the main app thread.

## Latency and User Experience Considerations

**Latency:** With a pure-Python Streamlit solution, expect some latency, but it can be kept reasonably low. The round-trip of audio capture -> network -> transcription -> display usually introduces a delay on the order of **a few hundred milliseconds to \~1 second**, depending on the setup:

* **Audio frame pipeline:** WebRTC frames are typically small (e.g. 20–30ms of audio each). You might batch a few frames (say 200–500ms of audio) before running a recognition step. In the DeepSpeech example, frames are gathered in a loop and processed continuously; the `intermediateDecode()` produces partial text as soon as some audio is fed. This means the text updates are near-instantaneous after each chunk of audio. DeepSpeech (and Vosk) can decode audio in realtime (or faster) on CPU for small models, so partial words appear with minimal delay (though finalizing longer words may take more context). Whisper models, if run locally, could introduce more delay especially on CPU or with larger models – you might only get output after each phrase or every few seconds unless using a very fast model/hardware.

* **Cloud STT latency:** Cloud services often use streaming protocols optimized for low latency. Azure’s service, for example, can start returning partial words almost immediately after you speak them. Network latency (to the Azure server) adds a bit of overhead (tens of ms). In practice, users of Azure’s streaming STT report very fast interim results (sometimes <300ms behind the speech). Google’s is similarly quick. The trade-off is you need internet connectivity and there’s API latency variability, but generally these services aim for real-time performance.

* **UI update frequency:** Streamlit can update elements quite rapidly (multiple times per second) if using an iterative loop or callbacks. The `text_output.markdown("**Text:** ...")` in the demo runs inside a loop that executes continuously. Each iteration updates the text placeholder. Streamlit’s front-end will redraw the text element with the new content. This happens seamlessly as long as the loop yields control periodically (e.g. via sleeping or waiting for frames). In the DeepSpeech demo, they even update a status message (“No frame arrived”, “Loading…”, “Running. Say something!”) in real time. So the UI can reflect the state promptly. **However**, extremely high-frequency updates (dozens per second) might overwhelm the app or cause flicker. In practice, updating the transcript a few times per second (or whenever new words are recognized) provides a smooth experience.

* **Partial vs Final Transcripts:** For a good UX, you’ll likely show *partial* (interim) transcripts as the user speaks, then maybe replace or finalize them once the user finishes a sentence. Many STT engines provide interim results that get refined. For example, Azure’s SDK can fire events for intermediate results (“hypotheses”) and a final result when the utterance ends. DeepSpeech’s `intermediateDecode()` gives a running transcript that updates with each new audio chunk. Users might notice the text updating word by word (or letter by letter in some engines), sometimes with minor corrections (the last few words might change once the full context is processed). This is normal in live transcription interfaces. It’s wise to indicate that the transcription is in progress (e.g. a blinking indicator or “Listening…” status) to set expectations. Once the user stops speaking, the final text can be shown (with proper punctuation/casing if the model provides it).

* **User experience limitations:** One limitation in Streamlit is that the app **reruns from the top on each interaction** by default. Using a continuous audio stream is not a typical “user interaction” (it’s more of a background event), so you have to manage the app loop carefully. The streamlit-webrtc component **avoids triggering full reruns on each audio frame** – instead, you handle frames in the callback or an internal loop. If you have other widgets on the page (buttons, text inputs, etc.), their events could still cause reruns that interrupt your transcription loop. A common pattern is to encapsulate the streaming logic in a section of the app and use session state or a placeholder to preserve the ongoing transcript between reruns. The example code above uses `if not webrtc_ctx.state.playing: return` to ensure the loop only runs when streaming is active. It also uses `st.empty()` placeholders for status and text output to update in place without clearing the whole app state.

* **Resource usage:** Real-time transcription can be CPU/GPU intensive. Running Whisper large in real-time on CPU, for instance, may lag noticeably. You’d likely use a smaller model or ensure a GPU is available. The app’s responsiveness could suffer if the STT processing blocks the main thread too much. Using asynchronous calls or threads to do the heavy lifting is important to keep audio streaming smooth. If the STT processing can’t keep up (e.g., decoding slower than real-time), you may experience buffering and increased latency or dropped audio frames. To mitigate this, consider processing shorter audio chunks or using faster models/engines.

## Example Architectures and Repositories

**1. Streamlit-webrtc + DeepSpeech demo:** This open-source example (by the `streamlit-webrtc` author) demonstrates the architecture for real-time STT. The app uses `webrtc_streamer` in send-only audio mode, and upon starting, it loads a DeepSpeech model and enters a loop receiving audio frames. Audio is accumulated and fed into DeepSpeech’s streaming interface. Partial text is retrieved with each new audio chunk and displayed live on the page via a `st.empty()` placeholder. Notably, **no external API** is used – it’s all local Python code. This repo can be a great reference for handling audio frames, model inference, and UI updates in sync. Key patterns from this example:

* Use **`WebRtcMode.SENDONLY`** (audio only) to get microphone input.
* Set **`audio_receiver_size`** (buffer size in frames) to control how many audio frames are buffered before processing. In the code it’s 1024, and then they fetch frames in batches.
* Acquire frames via `webrtc_ctx.audio_receiver.get_frames(timeout=…)` in a loop.
* Convert frames to a consistent format (mono 16k PCM) using PyDub/NumPy.
* Feed the audio to the STT engine’s stream and get **intermediate results** continuously.
* Update a Streamlit element (here, `text_output.markdown`) inside the loop for live feedback.
* Break out of the loop cleanly when streaming stops (e.g., if the user clicks “Stop” or leaves) by monitoring `webrtc_ctx.state.playing`.

*Code snippet (for illustration)* – from the DeepSpeech Streamlit app:

```python
stream = model.createStream()
status_indicator.write("Model loaded.")
while True:
    audio_frames = webrtc_ctx.audio_receiver.get_frames(timeout=1)
    # ... assemble frames into sound_chunk ...
    stream.feedAudioContent(buffer)
    text = stream.intermediateDecode()
    text_output.markdown(f"**Text:** {text}")
```

This yields an on-screen **transcription that updates in near real-time as you speak**. (In practice, you’d also handle exiting the loop and finalizing the result.)

**2. Azure Speech SDK example:** Another pattern is using a cloud STT in tandem with `streamlit-webrtc`. One community member shared code for streaming mic audio to Azure’s SDK. They set up an `AudioConfig` with a **PushAudioInputStream**, and each incoming audio frame from the webrtc callback was written into this stream (after converting to the correct format). They then ran a background thread that called `recognize_once()` repeatedly to get transcriptions asynchronously. Each time Azure returned a result, they updated `st.session_state['recognized_text']`, and the app displayed that text. This approach effectively segments the speech by pauses (since `recognize_once` returns after a silence or end-of-speech). It’s a bit simpler to implement (one complete sentence at a time, rather than constantly updating every word). For truly word-by-word updates, one could use Azure’s continuous mode (`recognizer.start_continuous_recognition_async` with result callbacks) – you’d handle the callback by updating session state or a queue that the Streamlit app checks periodically. The Azure SDK will identify when a speaker pauses or when a phrase is done, at which point you might want to send the transcribed query to the LLM.

**3. Other implementations:** There are blog tutorials using Streamlit for voice apps, but many use a record-then-transcribe flow (not streaming). For instance, some use `st.audio_input` to record and then OpenAI Whisper API to transcribe after you finish speaking. Those are simpler but don’t give live feedback while talking. If your goal is a chat interface with live transcription feeding into an LLM, the streaming approach (like #1 or #2 above) provides a better UX – the user can see the query being captured in real time, similar to how voice assistants show interim captions.

**Open-source repos** relevant to this problem include: the **`whitphx/streamlit-webrtc`** repo (which contains the STT demo code) and **`whitphx/streamlit-stt-app`** (the standalone app repository for the DeepSpeech demo). These can be studied for end-to-end implementation. Also, check Streamlit community forum threads like *“Web microphone stream to LLM”* where users have shared code integrating `streamlit-webrtc` with Azure STT, and *“Real-time speech-to-text using browser microphone on Azure Web App”* which discusses the challenges and solutions for cloud deployment.

## Technical Challenges and Bottlenecks

Building this in Streamlit (Python-only) is definitely possible, but be aware of a few **limitations and bottlenecks**:

* **Browser Mic Access:** In a cloud or deployed environment, your Python code cannot directly access the user’s microphone (since it runs on a server). The only way is to use browser APIs (getUserMedia) – which means **some JavaScript under the hood is inevitable**. The key is using a Streamlit Component (like `streamlit-webrtc` or `st.audio_input`) so that *you* don’t have to write the JS. As one user noted, *“accessing the microphone through JavaScript in the browser seems to be the only option… but integrating JavaScript with Python in Streamlit for continuous streaming poses a challenge.”* They found older hacks (`streamlit-bokeh-events`, etc.) that weren’t robust for continuous audio. **Streamlit-webrtc addresses this by providing a ready-made JS interface**, but it’s still a relatively advanced component. You might need to configure a TURN server if your app is deployed behind strict NAT or on Streamlit Cloud (the demo uses a Twilio TURN server for reliability). This adds a minor setup complexity, but it’s documented in the `streamlit-webrtc` repo.

* **Streamlit’s execution model:** Normally, Streamlit runs your script top-to-bottom, then pauses until the next user interaction. A continuous loop (for streaming audio) doesn’t fit that model neatly. The solution in practice is to start the loop **after** the user triggers the stream (e.g., after clicking the Start button on the WebRTC component). In the DeepSpeech example, once `webrtc_ctx.state.playing` becomes True, the app enters an infinite loop to process audio frames. This loop *never returns* until streaming stops – effectively holding the script in a running state. Streamlit can still update the UI from within this loop (because you’re calling `st.write/markdown` in it), but you have to be careful not to block completely. Inserting small `time.sleep()` delays or using non-blocking frame reads (with timeouts) ensures the loop doesn’t hog the CPU. If you don’t manage this, you risk freezing the app. Also, because the loop is within the Streamlit script, no other widgets will be responsive until it exits. One way to mitigate that is to use **threads** or **asyncio**. For example, you could start an async task for transcription and immediately return control to Streamlit, while that task updates session state. The UI could then use `st.session_state` to display updates. The Azure example uses a separate thread for the recognizer, letting the main thread remain free to update the interface (and respond to a stop button, etc.). Just remember to use thread-safe practices (updating Streamlit from a background thread should be done via session state or `st.experimental_rerun`, because calling `st.write` directly in a thread may not work).

* **UI update rate & synchronization:** There’s a natural tension between streaming speed and UI refresh. If you update the UI too often (say on every 20ms frame), performance will suffer. It’s wise to batch or throttle updates – e.g., update the transcript after each chunk of \~0.5 sec of audio or when a new word is recognized. In the demo, they update text once per batch of frames (the batch size is configurable). You also need to consider how to **synchronize final results**. Perhaps you’ll accumulate the full transcript as the user speaks and then, on a pause, finalize it (maybe send it to the LLM). You might maintain an internal buffer of the transcript so far. Some STT engines (Whisper, DeepSpeech) don’t automatically add punctuation or casing in interim results – finalizing the text might involve an extra step or simply using the engine’s final result output (which might be more polished). Azure and Google do add punctuation at final results, which is nice for sending to an LLM prompt directly.

* **Session state and reruns:** Using `st.session_state` is often crucial in this kind of app. For example, you can have `session_state['transcript']` that the background process updates. In your main app code, you would `st.write(session_state['transcript'])` continuously (or use an `st.empty` placeholder as a text box). If the Streamlit app happens to rerun (due to some other widget change), you should ensure it doesn’t wipe out the ongoing transcript. Marking the widget interactions with `st.experimental_pause` is not available, but you can structure your code to detect if a transcription thread is active and resume showing the live text. **Tip:** Use a dedicated “Start/Stop recording” button or toggle that, when turned on, launches the streaming + transcription process, and when off (or on rerun when it’s off) stops it. This way you control the lifecycle. `streamlit-webrtc` provides the start/stop via its UI, but you can also bind that to session state.

* **Concurrency & performance:** Python’s GIL means that CPU-bound STT processing might block other Python operations. If you’re using an offline recognizer in the same process, consider running it in a separate thread or even separate process if very heavy. For instance, Whisper large model decoding while also handling dozens of audio frames per second could be too slow. If using a thread, the GIL will prevent true parallelism on CPU – so you might not actually process audio while receiving new frames concurrently. One workaround is to use the async callback in `streamlit-webrtc` (`queued_audio_frames_callback`) to offload frames quickly and return immediately (perhaps returning silent frames, as shown in the code). Then process the queued frames outside the callback. This ensures the WebRTC component isn’t starved. Another approach: for heavy models, run the STT in a separate process or microservice (not strictly “pure Python in one app”, but still no manual JS). The Streamlit app could send audio to that service (even via a local socket or multiprocessing pipe) and get results back asynchronously. This adds complexity but can bypass GIL issues.

* **Stability and error handling:** Real-time apps need to handle things like the user not saying anything (idle mic), or long pauses, or the user clicking Stop. The code should break out of loops when appropriate. In the DeepSpeech example, they check if `audio_receiver` is None or no frames arrived to decide whether to continue or stop. Similarly, if using Azure’s continuous mode, you’d handle the “session stopped” event. Testing under various network conditions (especially if requiring TURN servers) is important, as WebRTC can sometimes disconnect or have startup latency. Ensure your app doesn’t crash if the audio stream ends unexpectedly.

**Bottom line:** *It is possible* to build a **browser-based chat interface with real-time voice transcription** in Streamlit using only Python libraries, but it requires combining the right tools. **Streamlit-webrtc is essential** for live audio capture in-browser. On the transcription side, you have flexibility: you can use local models like Whisper/Vosk/DeepSpeech for an all-Python solution, or call out to cloud STT services (Azure, Google, etc.) via their Python SDKs. The live transcription can be displayed on the Streamlit app by updating elements in a loop or callback. Expect a small latency in transcription, but with careful optimizations the experience can feel real-time (partial words appearing almost as they are spoken).

Finally, note that some developers opt for a **browser-side solution (Web Speech API)** to avoid these hassles, but that involves custom JS which you specifically want to avoid. Given that constraint, the methods described above are the current state-of-the-art for pure-Python Streamlit voice interfaces. With these tools and patterns, you can absolutely achieve a smooth voice-to-text streaming input feeding into your LLM prompts, all within Streamlit.

**Sources:** Real-time STT Streamlit demo with DeepSpeech; Streamlit forum discussions on mic streaming; Streamlit-webrtc documentation and examples.
